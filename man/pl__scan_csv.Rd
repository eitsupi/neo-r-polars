% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/input-csv-functions.R
\name{pl__scan_csv}
\alias{pl__scan_csv}
\title{Lazily read from a CSV file or multiple files via glob patterns}
\usage{
pl__scan_csv(
  source,
  ...,
  has_header = TRUE,
  separator = ",",
  comment_prefix = NULL,
  quote_char = "\\"",
  skip_rows = 0,
  schema = NULL,
  schema_overrides = NULL,
  null_values = NULL,
  missing_utf8_is_empty_string = FALSE,
  ignore_errors = FALSE,
  cache = FALSE,
  infer_schema = TRUE,
  infer_schema_length = 100,
  n_rows = NULL,
  encoding = c("utf8", "utf8-lossy"),
  low_memory = FALSE,
  rechunk = FALSE,
  skip_rows_after_header = 0,
  row_index_name = NULL,
  row_index_offset = 0,
  try_parse_dates = FALSE,
  eol_char = "\\n",
  raise_if_empty = TRUE,
  truncate_ragged_lines = FALSE,
  decimal_comma = FALSE,
  glob = TRUE,
  storage_options = NULL,
  retries = 2,
  file_cache_ttl = NULL,
  include_file_paths = NULL
)
}
\arguments{
\item{source}{Path(s) to a file or directory. When needing to authenticate
for scanning cloud locations, see the \code{storage_options} parameter.}

\item{...}{These dots are for future extensions and must be empty.}

\item{has_header}{Indicate if the first row of dataset is a header or not.If
\code{FALSE}, column names will be autogenerated in the following format:
\code{"column_x"} with \code{x} being an enumeration over every column in the dataset
starting at 1.}

\item{separator}{Single byte character to use as separator in the file.}

\item{comment_prefix}{A string, which can be up to 5 symbols in length, used
to indicate the start of a comment line. For instance, it can be set to \verb{#}
or \verb{//}.}

\item{quote_char}{Single byte character used for quoting. Set to \code{NULL} to
turn off special handling and escaping of quotes.}

\item{skip_rows}{Start reading after a particular number of rows. The header
will be parsed at this offset.}

\item{schema}{Provide the schema. This means that polars doesn't do schema
inference. This argument expects the complete schema, whereas
\code{schema_overrides} can be used to partially overwrite a schema. This must be
a list. Names of list elements are used to match to inferred columns.}

\item{schema_overrides}{Overwrite dtypes during inference. This must be a
list. Names of list elements are used to match to inferred columns.}

\item{null_values}{Character vector specifying the values to interpret as
\code{NA} values. It can be named, in which case names specify the columns in
which this replacement must be made (e.g. \code{c(col1 = "a")}).}

\item{missing_utf8_is_empty_string}{By default, a missing value is considered
to be \code{NA}. Setting this parameter to \code{TRUE} will consider missing UTF8
values as an empty character.}

\item{ignore_errors}{Keep reading the file even if some lines yield errors.
You can also use \code{infer_schema = FALSE} to read all columns as UTF8 to
check which values might cause an issue.}

\item{cache}{Cache the result after reading.}

\item{infer_schema}{If \code{TRUE} (default), the schema is inferred from the
data using the first \code{infer_schema_length} rows. When \code{FALSE}, the schema is
not inferred and will be \code{pl$String} if not specified in \code{schema} or
\code{schema_overrides}.}

\item{infer_schema_length}{The maximum number of rows to scan for schema
inference. If \code{NULL}, the full data may be scanned (this is slow). Set
\code{infer_schema = FALSE} to read all columns as \code{pl$String}.}

\item{n_rows}{Stop reading from the source after reading \code{n_rows}.}

\item{encoding}{Either \code{"utf8"} or \code{"utf8-lossy"}. Lossy means that invalid
UTF8 values are replaced with "?" characters.}

\item{low_memory}{Reduce memory pressure at the expense of performance.}

\item{rechunk}{Reallocate to contiguous memory when all chunks/files are parsed.}

\item{skip_rows_after_header}{Skip this number of rows when the header is
parsed.}

\item{row_index_name}{If not \code{NULL}, this will insert a row index column with
the given name.}

\item{row_index_offset}{Offset to start the row index column (only used if
the name is set by \code{row_index_name}).}

\item{try_parse_dates}{Try to automatically parse dates. Most ISO8601-like
formats can be inferred, as well as a handful of others. If this does not
succeed, the column remains of data type \code{pl$String}.}

\item{eol_char}{Single byte end of line character (default: \code{"\\n"}). When
encountering a file with Windows line endings (\code{"\\r\\n"}), one can go with the
default \code{"\\n"}. The extra \code{"\\r"} will be removed when processed.}

\item{raise_if_empty}{If \code{FALSE}, parsing an empty file returns an empty
DataFrame or LazyFrame.}

\item{truncate_ragged_lines}{Truncate lines that are longer than the schema.}

\item{decimal_comma}{Parse floats using a comma as the decimal separator
instead of a period.}

\item{glob}{Expand path given via globbing rules.}

\item{storage_options}{Named vector containing options that indicate how to
connect to a cloud provider. The cloud providers currently supported are
AWS, GCP, and Azure.
See supported keys here:
\itemize{
\item \href{https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html}{aws}
\item \href{https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html}{gcp}
\item \href{https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html}{azure}
\item Hugging Face (\verb{hf://}): Accepts an API key under the token parameter
\code{c(token = YOUR_TOKEN)} or by setting the \code{HF_TOKEN} environment
variable.
}

If \code{storage_options} is not provided, Polars will try to infer the
information from environment variables.}

\item{retries}{Number of retries if accessing a cloud instance fails.}

\item{file_cache_ttl}{Amount of time to keep downloaded cloud files since
their last access time, in seconds. Uses the \code{POLARS_FILE_CACHE_TTL}
environment variable (which defaults to 1 hour) if not given.}

\item{include_file_paths}{Include the path of the source file(s) as a column
with this name.}
}
\value{
A polars \link{LazyFrame}
}
\description{
This allows the query optimizer to push down predicates and projections to
the scan level, thereby potentially reducing memory overhead.
}
\examples{
my_file <- tempfile()
write.csv(iris, my_file)
lazy_frame <- pl$scan_csv(my_file)
lazy_frame$collect()
unlink(my_file)
}
