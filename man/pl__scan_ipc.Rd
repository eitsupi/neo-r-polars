% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/input-ipc-functions.R
\name{pl__scan_ipc}
\alias{pl__scan_ipc}
\title{Lazily read from an Arrow IPC (Feather v2) file or multiple files via glob
patterns}
\usage{
pl__scan_ipc(
  source,
  ...,
  n_rows = NULL,
  cache = TRUE,
  rechunk = FALSE,
  row_index_name = NULL,
  row_index_offset = 0L,
  storage_options = NULL,
  retries = 2,
  file_cache_ttl = NULL,
  hive_partitioning = NULL,
  hive_schema = NULL,
  try_parse_hive_dates = TRUE,
  include_file_paths = NULL
)
}
\arguments{
\item{source}{Path(s) to a file or directory. When needing to authenticate
for scanning cloud locations, see the \code{storage_options} parameter.}

\item{...}{These dots are for future extensions and must be empty.}

\item{n_rows}{Stop reading from parquet file after reading \code{n_rows}.}

\item{cache}{Cache the result after reading.}

\item{rechunk}{In case of reading multiple files via a glob pattern rechunk
the final DataFrame into contiguous memory chunks.}

\item{row_index_name}{If not \code{NULL}, this will insert a row index column with
the given name into the DataFrame.}

\item{row_index_offset}{Offset to start the row index column (only used if
the name is set).}

\item{storage_options}{Named vector containing options that indicate how to
connect to a cloud provider. The cloud providers currently supported are
AWS, GCP, and Azure.
See supported keys here:
\itemize{
\item \href{https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html}{aws}
\item \href{https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html}{gcp}
\item \href{https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html}{azure}
\item Hugging Face (\verb{hf://}): Accepts an API key under the token parameter
\code{c(token = YOUR_TOKEN)} or by setting the \code{HF_TOKEN} environment
variable.
}

If \code{storage_options} is not provided, Polars will try to infer the
information from environment variables.}

\item{retries}{Number of retries if accessing a cloud instance fails.}

\item{hive_partitioning}{Infer statistics and schema from Hive partitioned
sources and use them to prune reads. If \code{NULL} (default), it is automatically
enabled when a single directory is passed, and otherwise disabled.}

\item{hive_schema}{A list containing the column names and data types of the
columns by which the data is partitioned, e.g.
\code{list(a = pl$String, b = pl$Float32)}. If \code{NULL} (default), the schema of
the Hive partitions is inferred.}

\item{try_parse_hive_dates}{Whether to try parsing hive values as date /
datetime types.}

\item{include_file_paths}{Character value indicating the column name that
will include the path of the source file(s).}
}
\value{
A polars \link{LazyFrame}
}
\description{
This allows the query optimizer to push down predicates and projections to
the scan level, thereby potentially reducing memory overhead.
}
\examples{
\dontshow{if (requireNamespace("arrow", quietly = TRUE) && arrow::arrow_with_dataset()) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
temp_dir <- tempfile()
# Write a hive-style partitioned arrow file dataset
arrow::write_dataset(
  mtcars,
  temp_dir,
  partitioning = c("cyl", "gear"),
  format = "arrow",
  hive_style = TRUE
)
list.files(temp_dir, recursive = TRUE)

# If the path is a folder, Polars automatically tries to detect partitions
# and includes them in the output
pl$scan_ipc(temp_dir)$collect()

# We can also impose a schema to the partition
pl$scan_ipc(temp_dir, hive_schema = list(cyl = pl$String, gear = pl$Int32))$collect()
\dontshow{\}) # examplesIf}
}
