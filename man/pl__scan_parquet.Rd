% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/io-parquet-functions.R
\name{pl__scan_parquet}
\alias{pl__scan_parquet}
\title{Lazily read from a local or cloud-hosted parquet file (or files)}
\usage{
pl__scan_parquet(
  source,
  ...,
  n_rows = NULL,
  row_index_name = NULL,
  row_index_offset = 0L,
  parallel = c("auto", "columns", "row_groups", "prefiltered", "none"),
  use_statistics = TRUE,
  hive_partitioning = NULL,
  glob = TRUE,
  schema = NULL,
  hive_schema = NULL,
  try_parse_hive_dates = TRUE,
  rechunk = FALSE,
  low_memory = FALSE,
  cache = TRUE,
  storage_options = NULL,
  retries = 2,
  include_file_paths = NULL,
  allow_missing_columns = FALSE
)
}
\arguments{
\item{source}{Path(s) to a file or directory. When needing to authenticate
for scanning cloud locations, see the \code{storage_options} parameter.}

\item{...}{These dots are for future extensions and must be empty.}

\item{n_rows}{Stop reading from parquet file after reading \code{n_rows}.}

\item{row_index_name}{If not \code{NULL}, this will insert a row index column with
the given name into the DataFrame.}

\item{row_index_offset}{Offset to start the row index column (only used if
the name is set).}

\item{parallel}{This determines the direction and strategy of parallelism.
\code{"auto"} will try to determine the optimal direction. The \code{"prefiltered"}
strategy first evaluates the pushed-down predicates in parallel and
determines a mask of which rows to read. Then, it parallelizes over both the
columns and the row groups while filtering out rows that do not need to be
read. This can provide significant speedups for large files (i.e. many
row-groups) with a predicate that filters clustered rows or filters heavily.
In other cases, prefiltered may slow down the scan compared other strategies.

The prefiltered settings falls back to auto if no predicate is given.}

\item{use_statistics}{Use statistics in the parquet to determine if pages
can be skipped from reading.}

\item{hive_partitioning}{Infer statistics and schema from Hive partitioned
sources and use them to prune reads.}

\item{glob}{Expand path given via globbing rules.}

\item{schema}{Specify the datatypes of the columns. The datatypes must match
the datatypes in the file(s). If there are extra columns that are not in the
file(s), consider also enabling \code{allow_missing_columns}.}

\item{hive_schema}{The column names and data types of the columns by which
the data is partitioned. If \code{NULL} (default), the schema of the hive
partitions is inferred.}

\item{try_parse_hive_dates}{Whether to try parsing hive values as date /
datetime types.}

\item{rechunk}{In case of reading multiple files via a glob pattern rechunk
the final DataFrame into contiguous memory chunks.}

\item{low_memory}{Reduce memory pressure at the expense of performance}

\item{cache}{Cache the result after reading.}

\item{storage_options}{Named vector containing options that indicate how to
connect to a cloud provider. The cloud providers currently supported are
AWS, GCP, and Azure.
See supported keys here:
\itemize{
\item \href{https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html}{aws}
\item \href{https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html}{gcp}
\item \href{https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html}{azure}
\item Hugging Face (\verb{hf://}): Accepts an API key under the token parameter
\code{c(token = YOUR_TOKEN)} or by setting the \code{HF_TOKEN} environment
variable.
}

If \code{storage_options} is not provided, Polars will try to infer the
information from environment variables.}

\item{retries}{Number of retries if accessing a cloud instance fails.}

\item{include_file_paths}{Character value indicating the column name that
will include the path of the source file(s).}

\item{allow_missing_columns}{When reading a list of parquet files, if a
column existing in the first file cannot be found in subsequent files, the
default behavior is to raise an error. However, if \code{allow_missing_columns}
is set to \code{TRUE}, a full-NULL column is returned instead of erroring for the
files that do not contain the column.}
}
\value{
A polars \link{LazyFrame}
}
\description{
This allows the query optimizer to push down predicates and projections to
the scan level, thereby potentially reducing memory overhead.
}
\examples{
\dontshow{if (requireNamespace("withr", quietly = TRUE)) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
# Write a Parquet file than we can then import as DataFrame
temp_file <- withr::local_tempfile(fileext = ".parquet")
as_polars_df(mtcars)$write_parquet(temp_file)
pl$scan_parquet(temp_file)$collect()

# Write a hive-style partitioned parquet dataset
temp_dir <- withr::local_tempdir()
as_polars_df(mtcars)$write_parquet(temp_dir, partition_by = c("cyl", "gear"))
list.files(temp_dir, recursive = TRUE)

# If the path is a folder, Polars automatically tries to detect partitions
# and includes them in the output
pl$scan_parquet(temp_dir)$collect()
\dontshow{\}) # examplesIf}
}
