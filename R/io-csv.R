#' Lazily read from a CSV file or multiple files via glob patterns
#'
#' This allows the query optimizer to push down predicates and projections to
#' the scan level, thereby potentially reducing memory overhead.
#'
#' @rdname IO_scan_csv
#'
#' @inheritParams rlang::check_dots_empty0
#' @param source Path to a file or URL. It is possible to provide multiple paths
#' provided that all CSV files have the same schema. It is not possible to
#' provide several URLs.
#' @param has_header Indicate if the first row of dataset is a header or not.If
#' `FALSE`, column names will be autogenerated in the following format:
#' `"column_x"` with `x` being an enumeration over every column in the dataset
#' starting at 1.
#' @param separator Single byte character to use as separator in the file.
#' @param comment_prefix A string, which can be up to 5 symbols in length, used
#' to indicate the start of a comment line. For instance, it can be set to `#`
#' or `//`.
#' @param quote_char Single byte character used for quoting. Set to `NULL` to
#' turn off special handling and escaping of quotes.
#' @param skip_rows Start reading after a particular number of rows. The header
#' will be parsed at this offset.
#' @param schema Provide the schema. This means that polars doesn't do schema
#' inference. This argument expects the complete schema, whereas
#' `schema_overrides` can be used to partially overwrite a schema. This must
#' be a named list of column names - dtypes or dtype - column names. Supported
#' types so far are:
#' * "Boolean" or "logical" for DataType::Boolean,
#' * "Categorical" or "factor" for DataType::Categorical,
#' * "Float32" or "double" for DataType::Float32,
#' * "Float64" or "float64" for DataType::Float64,
#' * "Int32" or "integer" for DataType::Int32,
#' * "Int64" or "integer64" for DataType::Int64,
#' * "String" or "character" for DataType::String,
#' @param schema_overrides Overwrite dtypes during inference. This must be a
#' named list of column names - dtypes or dtype - column names. See `schema`
#' for supported types.
#' @param null_values Values to interpret as `NA` values. Can be:
#' * a character vector: all values that match one of the values in this vector
#'   will be `NA`;
#' * a named list with column names and null values.
#' @param missing_utf8_is_empty_string By default, a missing value is considered
#' to be `NA`. Setting this parameter to `TRUE` will consider missing UTF8
#' values as an empty character.
#' @param ignore_errors Keep reading the file even if some lines yield errors.
#' You can also use `infer_schema = FALSE` to read all columns as UTF8 to
#' check which values might cause an issue.
#' @param cache Cache the result after reading.
#' @param with_column_names Apply a function over the column names just in time
#' (when they are determined). This function will receive (and should return) a
#' list of column names.
#' @param infer_schema If `TRUE` (default), the schema is inferred from the
#' data using the first `infer_schema_length` rows. When `FALSE`, the schema is
#' not inferred and will be `pl$String` if not specified in `schema` or
#' `schema_overrides`.
#' @param infer_schema_length The maximum number of rows to scan for schema
#' inference. If `NULL`, the full data may be scanned (this is slow). Set
#' `infer_schema = FALSE` to read all columns as `pl$String`.
#' @param n_rows Stop reading from CSV file after reading `n_rows`.
#' @param encoding Either `"utf8"` or `"utf8-lossy"`. Lossy means that invalid
#' UTF8 values are replaced with "?" characters.
#' @param low_memory Reduce memory pressure at the expense of performance.
#' @param rechunk Reallocate to contiguous memory when all chunks / files are
#' parsed.
#' @param skip_rows_after_header Skip this number of rows when the header is
#' parsed.
#' @param row_index_name If not `NULL`, this will insert a row index column with
#' the given name into the DataFrame.
#' @param row_index_offset Offset to start the row index column (only used if
#' the name is set).
#' @param try_parse_dates Try to automatically parse dates. Most ISO8601-like
#' formats can be inferred, as well as a handful of others. If this does not
#' succeed, the column remains of data type `pl$String`.
#' @param eol_char Single byte end of line character (default: `\n`). When
#' encountering a file with Windows line endings (`\r\n`), one can go with the
#' default `\n`. The extra `\r` will be removed when processed.
#' @param new_columns Provide an explicit list of string column names to use
#' (for example, when scanning a headerless CSV file). If the given list is
#' shorter than the width of the DataFrame the remaining columns will have
#' their original name.
#' @param raise_if_empty If `FALSE`, parsing an empty file returns an empty
#' DataFrame or LazyFrame.
#' @param truncate_ragged_lines Truncate lines that are longer than the schema.
#' @param decimal_comma Parse floats using a comma as the decimal separator
#' instead of a period.
#' @param glob Expand path given via globbing rules.
#' @param storage_options Options that indicate how to connect to a cloud
#' provider. The cloud providers currently supported are AWS, GCP, and Azure.
#' See supported keys here:
#' * [aws](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
#' * [gcp](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
#' * [azure](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
#' * Hugging Face (`hf://`): Accepts an API key under the token parameter
#'   `list(token = YOUR_TOKEN)` or by setting the `HF_TOKEN` environment
#'   variable.
#'
#' If `storage_options` is not provided, Polars will try to infer the
#' information from environment variables.
#' @param credential_provider Provide a function that can be called to provide
#' cloud storage credentials. The function is expected to return a dictionary
#' of credential keys along with an optional credential expiry time.
#' @param retries Number of retries if accessing a cloud instance fails.
#' @param file_cache_ttl Amount of time to keep downloaded cloud files since
#' their last access time, in seconds. Uses the `POLARS_FILE_CACHE_TTL`
#' environment variable (which defaults to 1 hour) if not given.
#' @param include_file_paths Include the path of the source file(s) as a column
#' with this name.
#'
#' @inherit as_polars_lf return
#' @examples
#' my_file <- tempfile()
#' write.csv(iris, my_file)
#' lazy_frame <- pl$scan_csv(my_file)
#' lazy_frame$collect()
#' unlink(my_file)
pl__scan_csv <- function(
    source,
    ...,
    has_header = TRUE,
    separator = ",",
    comment_prefix = NULL,
    quote_char = '"',
    skip_rows = 0,
    # schema = NULL,
    # schema_overrides = NULL,
    # null_values = NULL,
    missing_utf8_is_empty_string = FALSE,
    ignore_errors = FALSE,
    cache = FALSE,
    # with_column_names = NULL,
    # infer_schema = TRUE,
    infer_schema_length = 100,
    n_rows = NULL,
    encoding = c("utf8", "utf8-lossy"),
    low_memory = FALSE,
    rechunk = FALSE,
    skip_rows_after_header = 0,
    row_index_name = NULL,
    row_index_offset = 0,
    try_parse_dates = FALSE,
    eol_char = "\n",
    # new_columns = NULL,
    raise_if_empty = TRUE,
    truncate_ragged_lines = FALSE,
    decimal_comma = FALSE,
    glob = TRUE,
    # storage_options = NULL,
    # credential_provider = NULL,
    retries = 2,
    file_cache_ttl = NULL,
    include_file_paths = NULL) {
  wrap({
    check_dots_empty0(...)
    encoding <- arg_match0(encoding, values = c("utf8", "utf8-lossy"))

    # capture all args and modify some to match lower level function
    args <- as.list(environment())
    # args[["path"]] <- lapply(source, check_is_link, raise_error = TRUE)
    args[["path"]] <- args[["source"]]
    args[["source"]] <- NULL

    # dtypes: convert named list of DataType's to DataTypeVector obj
    # if (!is.null(args$dtypes)) {
    #   args$dtypes <- list_to_datatype_vector(args$dtypes)
    # }

    # # null_values: convert string or un/named  char vec into RNullValues obj
    # if (!is.null(args$null_values)) {
    #   nullvals <- args$null_values
    #   RNullValues <- (function() {
    #     # one string is used as one NULL marker for all columns
    #     if (is_string(nullvals)) {
    #       return(RPolarsRNullValues$new_all_columns(nullvals))
    #     }

    #     # many unnamed strings(char vec) is used one mark for each column
    #     if (is.character(nullvals) && !is_named(nullvals)) {
    #       return(RPolarsRNullValues$new_columns(nullvals))
    #     }

    #     # named list is used as column(name) marker(value) pairs
    #     if (is.list(nullvals) && is_named(nullvals)) {
    #       return(RPolarsRNullValues$new_named(unlist(null_values)))
    #     }

    #     abort("`null_values` must be a string or a named list.")
    #   })

    #   args$null_values <- RNullValues
    # }

    if (is.null(row_index_name) && !is.null(row_index_offset)) {
      args["row_index_offset"] <- list(NULL)
    }

    ## call low level function with args
    # check_no_missing_args(new_from_csv, args)
    do.call(PlRLazyFrame$new_from_csv, args)
  })
}

#' New DataFrame from CSV
#' @rdname IO_read_csv
#' @inheritParams pl__scan_csv
#' @return [DataFrame][DataFrame_class]
pl__read_csv <- function(
    source,
    ...,
    has_header = TRUE,
    separator = ",",
    comment_prefix = NULL,
    quote_char = '"',
    skip_rows = 0,
    schema = NULL,
    schema_overrides = NULL,
    null_values = NULL,
    missing_utf8_is_empty_string = FALSE,
    ignore_errors = FALSE,
    cache = FALSE,
    with_column_names = NULL,
    infer_schema = TRUE,
    infer_schema_length = 100,
    n_rows = NULL,
    encoding = "utf8",
    low_memory = FALSE,
    rechunk = FALSE,
    skip_rows_after_header = 0,
    row_index_name = NULL,
    row_index_offset = 0,
    try_parse_dates = FALSE,
    eol_char = "\n",
    new_columns = NULL,
    raise_if_empty = TRUE,
    truncate_ragged_lines = FALSE,
    decimal_comma = FALSE,
    glob = TRUE,
    storage_options = NULL,
    credential_provider = NULL,
    retries = 2,
    file_cache_ttl = NULL,
    include_file_paths = NULL) {
  .args <- as.list(environment())
  result({
    do.call(pl$scan_csv, .args)$collect()
  }) |>
    unwrap("in pl$read_csv():")
}

cache_temp_file <- new.env(parent = new.env())
check_is_link <- function(path, raise_error = FALSE) {
  # do nothing let path fail on rust side
  if (is.na(path)) {
    return(NULL)
  }
  if (!file.exists(path)) {
    con <- NULL

    # check if possible to open url connection
    assumed_schemas <- c("", "https://", "http://", "ftp://")
    for (i_schema in assumed_schemas) {
      if (!is.null(con)) break
      actual_url <- paste0(i_schema, path)
      suppressWarnings(
        tryCatch(
          {
            con <- url(actual_url, open = "rt")
          },
          error = function(e) {}
        )
      )
    }

    # try download file if valid url
    if (!is.null(con)) {
      close(con)
      if (is.null(cache_temp_file[[actual_url]])) {
        cache_temp_file[[actual_url]] <- tempfile()
      }

      path <- cache_temp_file[[actual_url]] # redirect path to tmp downloaded file
    } else {
      if (raise_error) {
        Err_plain(paste("failed to locate file at path/url:", path)) |> unwrap()
      }
      # do nothing let path fail on rust side
      path <- NULL
    }
  }

  path
}


list_to_datatype_vector <- function(x) {
  if (!is.list(x) || !is_named(x)) {
    Err_plain("could not interpret dtypes, must be a named list of DataTypes") |>
      unwrap()
  }
  datatype_vector <- RPolarsDataTypeVector$new() # mutable
  mapply(
    name = names(x),
    type = unname(x),
    FUN = function(name, type) {
      # convert possible string to datatype
      if (is_string(type)) {
        type <- DataType$new(type)
      }
      if (!inherits(type, "RPolarsDataType")) {
        Err_plain("arg dtypes must be a named list of dtypes or dtype names") |>
          unwrap()
      }
      datatype_vector$push(name, type)
    }
  )
  datatype_vector
}
