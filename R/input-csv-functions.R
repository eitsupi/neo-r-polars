# Input CSV functions: scan_csv, read_csv

#' Lazily read from a CSV file or multiple files via glob patterns
#'
#' This allows the query optimizer to push down predicates and projections to
#' the scan level, thereby potentially reducing memory overhead.
#'
#' @inheritParams rlang::args_dots_empty
#' @param source Path to a file or URL. It is possible to provide multiple paths
#' provided that all CSV files have the same schema. It is not possible to
#' provide several URLs.
#' @param has_header Indicate if the first row of dataset is a header or not.If
#' `FALSE`, column names will be autogenerated in the following format:
#' `"column_x"` with `x` being an enumeration over every column in the dataset
#' starting at 1.
#' @param separator Single byte character to use as separator in the file.
#' @param comment_prefix A string, which can be up to 5 symbols in length, used
#' to indicate the start of a comment line. For instance, it can be set to `#`
#' or `//`.
#' @param quote_char Single byte character used for quoting. Set to `NULL` to
#' turn off special handling and escaping of quotes.
#' @param skip_rows Start reading after a particular number of rows. The header
#' will be parsed at this offset.
#' @param schema Provide the schema. This means that polars doesn't do schema
#' inference. This argument expects the complete schema, whereas
#' `schema_overrides` can be used to partially overwrite a schema. This must be
#' a list. Names of list elements are used to match to inferred columns.
#' @param schema_overrides Overwrite dtypes during inference. This must be a
#' list. Names of list elements are used to match to inferred columns.
#' @param null_values Character vector specifying the values to interpret as
#' `NA` values. It can be named, in which case names specify the columns in
#' which this replacement must be made (e.g. `c(col1 = "a")`).
#' @param missing_utf8_is_empty_string By default, a missing value is considered
#' to be `NA`. Setting this parameter to `TRUE` will consider missing UTF8
#' values as an empty character.
#' @param ignore_errors Keep reading the file even if some lines yield errors.
#' You can also use `infer_schema = FALSE` to read all columns as UTF8 to
#' check which values might cause an issue.
#' @param cache Cache the result after reading.
#'
#  TODO: enable this parameter
#  @param with_column_names Apply a function over the column names just in time
#  (when they are determined). This function will receive (and should return) a
#  list of column names.
#'
#' @param infer_schema If `TRUE` (default), the schema is inferred from the
#' data using the first `infer_schema_length` rows. When `FALSE`, the schema is
#' not inferred and will be `pl$String` if not specified in `schema` or
#' `schema_overrides`.
#' @param infer_schema_length The maximum number of rows to scan for schema
#' inference. If `NULL`, the full data may be scanned (this is slow). Set
#' `infer_schema = FALSE` to read all columns as `pl$String`.
#' @param n_rows Stop reading from CSV file after reading `n_rows`.
#' @param encoding Either `"utf8"` or `"utf8-lossy"`. Lossy means that invalid
#' UTF8 values are replaced with "?" characters.
#' @param low_memory Reduce memory pressure at the expense of performance.
#' @param rechunk Reallocate to contiguous memory when all chunks / files are
#' parsed.
#' @param skip_rows_after_header Skip this number of rows when the header is
#' parsed.
#' @param row_index_name If not `NULL`, this will insert a row index column with
#' the given name into the DataFrame.
#' @param row_index_offset Offset to start the row index column (only used if
#' the name is set).
#' @param try_parse_dates Try to automatically parse dates. Most ISO8601-like
#' formats can be inferred, as well as a handful of others. If this does not
#' succeed, the column remains of data type `pl$String`.
#' @param eol_char Single byte end of line character (default: `\n`). When
#' encountering a file with Windows line endings (`\r\n`), one can go with the
#' default `\n`. The extra `\r` will be removed when processed.
#'
#  TODO: enable this parameter (requires infrastructure for with_column_names)
#  @param new_columns Provide an explicit list of string column names to use
#  (for example, when scanning a headerless CSV file). If the given list is
#  shorter than the width of the DataFrame the remaining columns will have
#  their original name.
#'
#' @param raise_if_empty If `FALSE`, parsing an empty file returns an empty
#' DataFrame or LazyFrame.
#' @param truncate_ragged_lines Truncate lines that are longer than the schema.
#' @param decimal_comma Parse floats using a comma as the decimal separator
#' instead of a period.
#' @param glob Expand path given via globbing rules.
#' @param storage_options Named vector containing options that indicate how to
#' connect to a cloud provider. The cloud providers currently supported are
#' AWS, GCP, and Azure.
#' See supported keys here:
#' * [aws](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
#' * [gcp](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
#' * [azure](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
#' * Hugging Face (`hf://`): Accepts an API key under the token parameter
#'   `c(token = YOUR_TOKEN)` or by setting the `HF_TOKEN` environment
#'   variable.
#'
#' If `storage_options` is not provided, Polars will try to infer the
#' information from environment variables.
#' @param credential_provider Provide a function that can be called to provide
#' cloud storage credentials. The function is expected to return a dictionary
#' of credential keys along with an optional credential expiry time.
#' @param retries Number of retries if accessing a cloud instance fails.
#' @param file_cache_ttl Amount of time to keep downloaded cloud files since
#' their last access time, in seconds. Uses the `POLARS_FILE_CACHE_TTL`
#' environment variable (which defaults to 1 hour) if not given.
#' @param include_file_paths Include the path of the source file(s) as a column
#' with this name.
#'
#' @inherit as_polars_lf return
#' @examples
#' my_file <- tempfile()
#' write.csv(iris, my_file)
#' lazy_frame <- pl$scan_csv(my_file)
#' lazy_frame$collect()
#' unlink(my_file)
pl__scan_csv <- function(
  source,
  ...,
  has_header = TRUE,
  separator = ",",
  comment_prefix = NULL,
  quote_char = '"',
  skip_rows = 0,
  schema = NULL,
  schema_overrides = NULL,
  null_values = NULL,
  missing_utf8_is_empty_string = FALSE,
  ignore_errors = FALSE,
  cache = FALSE,
  infer_schema = TRUE,
  infer_schema_length = 100,
  n_rows = NULL,
  encoding = c("utf8", "utf8-lossy"),
  low_memory = FALSE,
  rechunk = FALSE,
  skip_rows_after_header = 0,
  row_index_name = NULL,
  row_index_offset = 0,
  try_parse_dates = FALSE,
  eol_char = "\n",
  raise_if_empty = TRUE,
  truncate_ragged_lines = FALSE,
  decimal_comma = FALSE,
  glob = TRUE,
  storage_options = NULL,
  retries = 2,
  file_cache_ttl = NULL,
  include_file_paths = NULL
) {
  check_dots_empty0(...)
  check_character(source, allow_na = FALSE)
  if (length(source) == 0) {
    abort("`source` must have length > 0.")
  }
  check_list_of_polars_dtype(schema_overrides, allow_null = TRUE)
  check_list_of_polars_dtype(schema, allow_null = TRUE)
  check_character(storage_options, allow_null = TRUE)
  check_character(null_values, allow_null = TRUE)
  encoding <- arg_match0(encoding, values = c("utf8", "utf8-lossy"))

  if (isFALSE(infer_schema)) {
    infer_schema_length <- 0
  }

  schema_overrides <- parse_into_list_of_datatypes(!!!schema_overrides)

  if (!is.null(schema)) {
    schema <- parse_into_list_of_datatypes(!!!schema)
  }

  PlRLazyFrame$new_from_csv(
    source = source,
    separator = separator,
    has_header = has_header,
    ignore_errors = ignore_errors,
    skip_rows = skip_rows,
    cache = cache,
    missing_utf8_is_empty_string = missing_utf8_is_empty_string,
    low_memory = low_memory,
    rechunk = rechunk,
    skip_rows_after_header = skip_rows_after_header,
    encoding = encoding,
    try_parse_dates = try_parse_dates,
    eol_char = eol_char,
    raise_if_empty = raise_if_empty,
    truncate_ragged_lines = truncate_ragged_lines,
    decimal_comma = decimal_comma,
    glob = glob,
    retries = retries,
    comment_prefix = comment_prefix,
    quote_char = quote_char,
    null_values = null_values,
    infer_schema_length = infer_schema_length,
    row_index_name = row_index_name,
    row_index_offset = row_index_offset,
    n_rows = n_rows,
    overwrite_dtype = schema_overrides,
    schema = schema,
    storage_options = storage_options,
    file_cache_ttl = file_cache_ttl,
    include_file_paths = include_file_paths
  ) |>
    wrap()
}

#' New DataFrame from CSV
#' @inheritParams pl__scan_csv
#' @inherit as_polars_df return
#' @examples
#' my_file <- tempfile()
#' write.csv(iris, my_file)
#' pl$read_csv(my_file)
#' unlink(my_file)
pl__read_csv <- function(
  source,
  ...,
  has_header = TRUE,
  separator = ",",
  comment_prefix = NULL,
  quote_char = '"',
  skip_rows = 0,
  schema = NULL,
  schema_overrides = NULL,
  null_values = NULL,
  missing_utf8_is_empty_string = FALSE,
  ignore_errors = FALSE,
  cache = FALSE,
  infer_schema = TRUE,
  infer_schema_length = 100,
  n_rows = NULL,
  encoding = c("utf8", "utf8-lossy"),
  low_memory = FALSE,
  rechunk = FALSE,
  skip_rows_after_header = 0,
  row_index_name = NULL,
  row_index_offset = 0,
  try_parse_dates = FALSE,
  eol_char = "\n",
  raise_if_empty = TRUE,
  truncate_ragged_lines = FALSE,
  decimal_comma = FALSE,
  glob = TRUE,
  storage_options = NULL,
  retries = 2,
  file_cache_ttl = NULL,
  include_file_paths = NULL
) {
  check_dots_empty0(...)
  .args <- as.list(environment())
  do.call(pl$scan_csv, .args)$collect() |>
    wrap()
}
